{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Example\n",
    "\n",
    "Here we will implement the Policy Evaluation, Value Iteration and Value Iteration algorithms\n",
    "\n",
    "Recall, the MDP we consider in module 1 is \n",
    "\n",
    "<img src=\"mdp-cs747.png\" \n",
    "     align=\"center\" \n",
    "     width=\"300\" />\n",
    "     \n",
    "For the above MDP, the transition probaility matrix is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s'|s,a = RED) = \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5 & 0 \\\\\n",
    "0.25 & 0 & 0.75\\\\\n",
    "0 & 0.5 & 0.5\n",
    "\\end{bmatrix}\n",
    ",~~~and~~~\n",
    "\\mathcal{P}(s'|s,a = BLUE) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The reward function is \n",
    "\n",
    "$$\n",
    "\\mathcal{R}(s'|s,a = RED) = \n",
    "\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 0 & -2\\\\\n",
    "0 & 3 & 3\n",
    "\\end{bmatrix}\n",
    ",~~~and~~~\n",
    "\\mathcal{R}(s'|s,a = BLUE) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "2 & 0 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We first implement the Policy Evaluation for various policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.99915359 10.99923823  9.99923823]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define MDP parameters\n",
    "num_states = 3  # [0,1,2]\n",
    "num_actions = 2 # [0,1]\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "# Define rewards and transition probabilities\n",
    "R = np.zeros((num_states, num_actions, num_states))\n",
    "R[0, 0, :] = [0,1,0]\n",
    "R[0, 1, :] = [1,0,0]\n",
    "R[1, 0, :] = [-1,0,-2]\n",
    "R[1, 1, :] = [2,0,0]\n",
    "R[2, 0, :] = [0,3,3]\n",
    "R[2, 1, :] = [1,0,0]\n",
    "\n",
    "# Define the transition probabilities for each state and action\n",
    "P = np.zeros((num_states, num_actions, num_states))\n",
    "P[0, 0, :] = [0.5, 0.5, 0]\n",
    "P[0, 1, :] = [1, 0, 0]\n",
    "P[1, 0, :] = [0.25, 0, 0.75]\n",
    "P[1, 1, :] = [1, 0, 0]\n",
    "P[2, 0, :] = [0, 0.5, 0.5]\n",
    "P[2, 1, :] = [1, 0, 0]\n",
    "\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "# Perform policy evaluation\n",
    "tolerance = 1e-6\n",
    "delta = np.inf\n",
    "\n",
    "def policy_evaluation(transitions, rewards, policy, gamma, theta=0.0001):\n",
    "    num_states, num_actions, _ = transitions.shape\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            value = 0\n",
    "            for action in range(num_actions):\n",
    "                action_prob = policy[s][action]\n",
    "                for next_state in range(num_states):\n",
    "                    prob = transitions[s][action][next_state]\n",
    "                    value += action_prob * prob * (rewards[s][action][next_state] + gamma * V[next_state])\n",
    "            delta = max(delta, abs(value - V[s]))\n",
    "            V[s] = value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Define policy\n",
    "policy = np.array([[0, 1], [0, 1], [0, 1]])  # deterministic policy\n",
    "## If the policy is RRB then the polcy will be written as [[1,0],[1,0], [0,1]]\n",
    "## If the policy is BRB then the polcy will be written as [[0,1],[1,0], [1,0]] etc.\n",
    "\n",
    "val = policy_evaluation(P, R, policy, gamma, theta=0.0001)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value function:\n",
      "[ 9.99174648 10.99257183 14.4478601 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the MDP parameters\n",
    "num_states = 3\n",
    "num_actions = 2\n",
    "\n",
    "# Define the transition probabilities for each state and action\n",
    "P = np.zeros((num_states, num_actions, num_states))\n",
    "P[0, 0, :] = [0.5, 0.5, 0]\n",
    "P[0, 1, :] = [1, 0, 0]\n",
    "P[1, 0, :] = [0.25, 0, 0.75]\n",
    "P[1, 1, :] = [1, 0, 0]\n",
    "P[2, 0, :] = [0, 0.5, 0.5]\n",
    "P[2, 1, :] = [1, 0, 0]\n",
    "\n",
    "# Define the reward function for each state and action\n",
    "R = np.zeros((num_states, num_actions, num_states))\n",
    "R[0, 0, :] = [0,1,0]\n",
    "R[0, 1, :] = [1,0,0]\n",
    "R[1, 0, :] = [-1,0,-2]\n",
    "R[1, 1, :] = [2,0,0]\n",
    "R[2, 0, :] = [0,3,3]\n",
    "R[2, 1, :] = [1,0,0]\n",
    "\n",
    "# Define the discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "epsilon = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the value function to zeros\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "# Run the value iteration algorithm\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in range(num_states):\n",
    "        v_old = V[s]\n",
    "        # Calculate the new value for each state\n",
    "        V[s] = max([sum([P[s, a, s1] * (R[s, a, s1] + gamma * V[s1]) for s1 in range(num_states)]) for a in range(num_actions)])\n",
    "        # Check for convergence\n",
    "        delta = max(delta, abs(V[s] - v_old))\n",
    "    if delta < epsilon:\n",
    "        break\n",
    "\n",
    "# Print the final value function\n",
    "print(\"Final value function:\")\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy:\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Optimal value function:\n",
      "[ 9.99918039 10.99926235 14.45388157]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the MDP parameters\n",
    "num_states = 3\n",
    "num_actions = 2\n",
    "\n",
    "# Define the transition probabilities for each state and action\n",
    "P = np.zeros((num_states, num_actions, num_states))\n",
    "P[0, 0, :] = [0.5, 0.5, 0]\n",
    "P[0, 1, :] = [1, 0, 0]\n",
    "P[1, 0, :] = [0.25, 0, 0.75]\n",
    "P[1, 1, :] = [1, 0, 0]\n",
    "P[2, 0, :] = [0, 0.5, 0.5]\n",
    "P[2, 1, :] = [1, 0, 0]\n",
    "\n",
    "# Define the reward function for each state and action\n",
    "R = np.zeros((num_states, num_actions, num_states))\n",
    "R[0, 0, :] = [0,1,0]\n",
    "R[0, 1, :] = [1,0,0]\n",
    "R[1, 0, :] = [-1,0,-2]\n",
    "R[1, 1, :] = [2,0,0]\n",
    "R[2, 0, :] = [0,3,3]\n",
    "R[2, 1, :] = [1,0,0]\n",
    "\n",
    "# Define the discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the policy\n",
    "policy = np.array([[1, 0], [1, 0], [1, 0]])\n",
    "\n",
    "V = np.zeros(num_states)\n",
    "tolerance = 1e-6\n",
    "delta = np.inf\n",
    "\n",
    "# policy = np.zeros((num_states, num_actions))\n",
    "# policy[:, 0] = 1\n",
    "\n",
    "## policy Evaluation\n",
    "\n",
    "def policy_evaluation(transitions, rewards, policy, gamma, theta=0.0001):\n",
    "    num_states, num_actions, _ = transitions.shape\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            value = 0\n",
    "            for action in range(num_actions):\n",
    "                action_prob = policy[s][action]\n",
    "                for next_state in range(num_states):\n",
    "                    prob = transitions[s][action][next_state]\n",
    "                    value += action_prob * prob * (rewards[s][action][next_state] + gamma * V[next_state])\n",
    "            delta = max(delta, abs(value - V[s]))\n",
    "            V[s] = value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "\n",
    "# Run policy iteration\n",
    "num_iterations = 20\n",
    "for i in range(num_iterations):\n",
    "    # Policy evaluation\n",
    "    val = policy_evaluation(P, R, policy, gamma, theta=0.0001)\n",
    "    # Policy improvement\n",
    "    policy_stable = True\n",
    "    for s in range(num_states):\n",
    "        old_action = np.argmax(policy[s])\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            action_values[a] = np.sum(R[s, a, :]*P[s, a, :] + gamma * val * P[s, a, :])\n",
    "        best_action = np.argmax(action_values)\n",
    "        if old_action != best_action:\n",
    "            policy_stable = False\n",
    "            policy[s, :] = 0\n",
    "            policy[s, best_action] = 1\n",
    "    if policy_stable:\n",
    "        break\n",
    "\n",
    "print(\"Optimal policy:\")\n",
    "print(policy)\n",
    "print(\"Optimal value function:\")\n",
    "print(V)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
